pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.
pandas is a NumFOCUS sponsored project. This will help ensure the success of development of pandas as a world-class open-source project

import pandas as pd
print(pd.__version__)
print(pd.show_versions(as_json=True))

output:
0.24.2
{'system': {'commit': None, 'python': '3.7.3.final.0', 'python-bits': 64, 'OS': 'Windows', 'OS-release': '10', 'machine': 'AMD64', 'processor': 'AMD64 Family 23 Model 24 Stepping 1, AuthenticAMD', 'byteorder': 'little', 'LC_ALL': 'None', 'LANG': 'None', 'LOCALE': 'None.None'}, 'dependencies': {'pandas': '0.24.2', 'pytest': '5.0.1', 'pip': '19.1.1', 'setuptools': '41.4.0', 'Cython': '0.29.12', 'numpy': '1.16.4', 'scipy': '1.2.1', 'pyarrow': None, 'xarray': None, 'IPython': '7.6.1', 'sphinx': '2.1.2', 'patsy': '0.5.1', 'dateutil': '2.8.0', 'pytz': '2019.1', 'blosc': None, 'bottleneck': '1.2.1', 'tables': '3.5.2', 'numexpr': '2.6.9', 'feather': None, 'matplotlib': '3.1.0', 'openpyxl': '2.6.2', 'xlrd': '1.2.0', 'xlwt': '1.3.0', 'xlsxwriter': '1.1.8', 'lxml.etree': '4.3.4', 'bs4': '4.7.1', 'html5lib': '1.0.1', 'sqlalchemy': '1.3.5', 'pymysql': None, 'psycopg2': '2.8.3 (dt dec pq3 ext lo64)', 'jinja2': '2.10.1', 's3fs': None, 'fastparquet': None, 'pandas_gbq': None, 'pandas_datareader': None, 'gcsfs': None}}
None

-------Create a pandas series: a list, numpy and a dictionary
mylist=list('abcedfghijklmnopqrstuvwxyz')
myarr=np.arange(26)
mydict=dict(zip(myarr,mylist)) #zip combines myarr[0],mylist[0] into a tuple
ser1=pd.Series(mylist)
ser2=pd.Series(myarr)
ser3=pd.Series(mydict)
ser3.name='Common' # name a series
print(ser1)
print(ser2)
print(ser3)

output:
0     a
1     b
2     c
3     e
.......
dtype: object
0      0
1      1
2      2
3      3
.......
dtype: int32
0     a
1     b
2     c
3     e
......
Name: Common, dtype: object

-------Convert the series ser into a dataframe with its index as another column on the dataframe
mylist=list('abcedfghijklmnopqrstuvwxyz')
myarr=np.arange(26)
mydict=dict(zip(myarr,mylist))
ser=pd.Series(mydict)
df=ser.to_frame().reset_index()
print(df)

output:
    index  0
0       0  a
1       1  b
2       2  c
3       3  e
.......

-------Combine ser1 and ser2 to form a dataframe
mylist=list('abcedfghijklmnopqrstuvwxyz')
myarr=np.arange(26)
mydict=dict(zip(myarr,mylist))
ser1=pd.Series(mylist)
ser2=pd.Series(myarr)
ser3=pd.Series(mydict)
#df=pd.concat([ser1,ser2],axis=1)
df=pd.DataFrame({'col1':ser1,'col2':ser2})
print(df)

output:
   col1  col2
0     a     0
1     b     1
2     c     2
3     e     3
.......

-------get the items of series A not present in series B
ser1=pd.Series([1,2,3,4,5])
ser2=pd.Series([4,5,6,7,8])
print(ser1[~ser1.isin(ser2)]) 
print(ser1[ser1.isin(ser2)]) # common items present in both

output:
0    1
1    2
2    3
dtype: int64
3    4
4    5
dtype: int64

-------items not common to both series A and series B
ser1 = pd.Series([1, 2, 3, 4, 5])
ser2 = pd.Series([4, 5, 6, 7, 8])
ser_u = pd.Series(np.union1d(ser1, ser2))  # union
ser_i = pd.Series(np.intersect1d(ser1, ser2))  # intersect
print(ser_u[~ser_u.isin(ser_i)])

output:
0    1
1    2
2    3
5    6
6    7
7    8
dtype: int64

-------minimum, 25th percentile, median, 75th, and max of a numeric series
state=np.random.RandomState(100) 
ser=pd.Series(state.normal(1,5,25))
print(np.percentile(ser,q=[0,25,50,75,100])) #percentiles

output:
[-7.74882737 -1.29013493  1.92259345  4.36360403  9.0949083 ]

-------frequency counts of each unique value ser
ser=pd.Series(np.take(list('abcdefgh'),np.random.randint(8,size=30))) #generates 30 values from list
print(ser.value_counts())

output:
b    7
a    6
g    6
f    3
c    3
e    2
h    2
d    1
dtype: int64

------- keep the top 2 most frequent items as it is and replace everything else as ‘Other’.
np.random.seed(1)
ser=pd.Series(np.random.randint(1,10,[12]))
#print(ser.value_counts())
ser[~ser.isin(ser.value_counts().index[:2])]='Other'
print(ser)

output:
0         6
1     Other
2         6
3     Other
4     Other
5     Other
6     Other
7     Other
8         3
9     Other
10        6
11        3
dtype: object

-------Bin the series ser into 10 equal deciles and replace the values with the bin name.
ser = pd.Series(np.random.random(20))
print(ser.head())
pd.qcut(ser, q=[0, .10, .20, .3, .4, .5, .6, .7, .8, .9, 1], 
        labels=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']).head()

output:
0    0.524548
1    0.443453
2    0.229577
3    0.534414
4    0.913962
dtype: float64
0     5th
1     3rd
2     2nd
3     5th
4    10th
dtype: category
Categories (10, object): [1st < 2nd < 3rd < 4th ... 7th < 8th < 9th < 10th]

-------Reshape the series ser into a dataframe with 7 rows and 5 columns
np.random.seed(2)
ser=pd.Series(np.random.randint(1,10,35))
df=pd.DataFrame(ser.values.reshape(7,5))
print(df)

output:
   0  1  2  3  4
0  9  9  7  3  9
1  8  3  2  6  5
2  5  6  8  4  7
3  5  4  8  7  2
4  4  6  9  5  7
5  4  3  1  5  3
6  5  2  8  9  3

-------positions of numbers that are multiples of 3 from ser
np.random.seed(2)
ser=pd.Series(np.random.randint(1,10,35))
print(ser)
print(np.argwhere(ser%3==0))

output:


-------From ser, extract the items at positions in list pos
np.random.seed(2)
ser=pd.Series(list('abcdefghijklmnopqrstuvwxyz'))
pos = [0,4,8]
print(ser.take(pos))

output:
0    a
4    e
8    i
dtype: object

-------Stack ser1 and ser2 vertically and horizontally (to form a dataframe)
ser1=pd.Series(range(5))
ser2=pd.Series(list('abcde'))
ser3=pd.Series(range(2))
print(ser1.append(ser2.append(ser3))) # append vertically
print(pd.concat([ser1,ser2],axis=1)) # append horizontally in dataframe

output:
0    0
1    1
2    2
3    3
4    4
0    a
1    b
2    c
3    d
4    e
0    0
1    1
dtype: object
   0  1
0  0  a
1  1  b
2  2  c
3  3  d
4  4  e

-------Get the positions of items of ser2 in ser1 as a list.
ser1 = pd.Series([10, 9, 6, 5, 3, 1, 12, 8, 13])
ser2 = pd.Series([1, 3, 10, 13])
a=[pd.Index(ser1).get_loc(i) for i in ser2]
print(a)

output:
[5, 4, 0, 8]

-------mean squared error of truth and pred series.
np.random.seed(2)
truth=pd.Series(range(10))
pred=pd.Series(range(10))+np.random.random(10)
print(pred)
print(np.mean(truth-pred)**2)

output:
0    0.435995
1    1.025926
2    2.549662
3    3.435322
4    4.420368
5    5.330335
6    6.204649
7    7.619271
8    8.299655
9    9.266827
dtype: float64
0.12873817028275983

-------Change the first character of each word to upper case in each word of ser
ser=pd.Series(['how','to','kick','ass'])
#print(pd.Series([i.title() for i in ser])) # solution1
#print(ser.map(lambda x: x.title())) # solution 2
print(ser.map(lambda x: x[0].upper()+x[1:])) # solution 3

output:
0     How
1      To
2    Kick
3     Ass

-------calculate the number of characters in each word in a series
ser=pd.Series(['how','to','kick','ass'])
print(ser.map(lambda x: len(x))) 

output:
0    3
1    2
2    4
3    3
dtype: int64

-------Difference of differences between the consequtive numbers of ser.
ser=pd.Series([1, 3, 6, 10, 15, 21, 27, 35])
print(ser.diff().tolist())
print(ser.diff().diff().tolist())

output:

[nan, 2.0, 3.0, 4.0, 5.0, 6.0, 6.0, 8.0]
[nan, nan, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0]

-------convert a series of date-strings to a timeseries
ser=pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])
#pd.to_datetime(ser) # solution 1
from dateutil.parser import parse
ser.map(lambda x:parse(x))
#Get the day of month, week number, day of year and day of week from ser
ser_ts=ser.map(lambda x:parse(x))
print('Date: ',ser_ts.dt.day.tolist())
print('Week number',ser_ts.dt.weekofyear.tolist())
print('Day number of year: ',ser_ts.dt.dayofyear.tolist())
print('Day of week: ',ser_ts.dt.weekday_name.tolist())

output:
Date:  [1, 2, 3, 4, 5, 6]
Week number [53, 5, 9, 14, 19, 23]
Day number of year:  [1, 33, 63, 94, 125, 157]
Day of week:  ['Friday', 'Wednesday', 'Saturday', 'Thursday', 'Monday', 'Saturday']

-------convert year-month string to dates corresponding to the 4th day of the month
ser=pd.Series(['Jan 2010', 'Feb 2011', 'Mar 2012'])
from dateutil.parser import parse
#ser_ts=ser.map(lambda x:parse('04'+x)) #solution 1
#print(ser_ts)
ser_ts=ser.map(lambda x:parse(x))
#solution2
ser_dtstr=ser_ts.dt.year.astype('str')+'-'+ser_ts.dt.month.astype('str')+'-'+'04'
print([parse(i).strftime('%Y-%m-%d') for i in ser_dtstr])

output:
['2010-01-04', '2011-02-04', '2012-03-04']

-------mean of weights of each fruit
np.random.seed(2)
fruit=pd.Series(np.random.choice(['apple', 'banana', 'carrot'],10)) #generate 10 values of fruits
weights=pd.Series(np.linspace(1,10,10)) # generates 10 values from 1-10 random
print(weights.groupby(fruit).mean())

output:
apple     3.333333
banana    6.333333
carrot    6.500000
dtype: float64

-------euclidean distance 
p=pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
q=pd.Series([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])
print(sum((p-q)**2)**0.5) # ((p-q) whole square)under root
print(np.linalg.norm(p-q)) # formula

output:
18.16590212458495
18.16590212458495

-------positions of local maxima (or peaks) in a numeric series, (values surrounded by smaller values on both sides
ser=pd.Series([2, 10, 3, 4, 9, 10, 2, 7, 3])
#sign function returns -1 if x < 0, 0 if x==0, 1 if x > 0.
dd=np.diff(np.sign(np.diff(ser)))
peaks=np.where(dd==-2)[0]+1
print(peaks)

output:
[1 5 7]

-------Replace the spaces in my_str with the least frequent character
my_str='dbc deb abed gade'
ser=pd.Series(list(my_str))
freq=ser.value_counts()
#print(freq)
least_freq=freq.dropna().index[-1] # to find least occurance variable
print(''.join(ser.replace(' ',least_freq)))

output:
dbccdebcabedcgade

-------TimeSeries starting ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values
ser=pd.Series(np.random.randint(1,10,10),pd.date_range('2000-01-01',periods=10,freq='W-SAT'))
print(ser)

output:
2000-01-01    5
2000-01-08    5
2000-01-15    6
2000-01-22    8
2000-01-29    4
2000-02-05    7
2000-02-12    5
2000-02-19    4
2000-02-26    8
2000-03-04    7
Freq: W-SAT, dtype: int32

-------Make all missing dates appear and fill up with value from previous date.
ser=pd.Series([1,10,3, np.nan], index=pd.to_datetime(['2000-01-01', '2000-01-03', '2000-01-06', '2000-01-08']))
#print(ser.resample('D').ffill()) # fill with previous value
#print(ser.resample('D').bfill()) # fill with next value
print(ser.resample('D').bfill().ffill()) #fill next else prev value

output:
2000-01-01     1.0
2000-01-02    10.0
2000-01-03    10.0
2000-01-04     3.0
2000-01-05     3.0
2000-01-06     3.0
2000-01-07     3.0
2000-01-08     3.0

-------Compute autocorrelations for the first 10 lags of ser. Find out which lag has the largest correlation
ser=pd.Series(np.arange(20)+np.random.normal(1,10,20)) #normal = normal distribution
autocor=[ser.autocorr(i).round(2) for i in range(11)]
print(autocor[1:])
print('Lag having highest correlation: ',np.argmax(np.abs(autocor[1:]))+1)

output:
[0.06, 0.3, -0.03, 0.51, 0.45, 0.08, -0.09, -0.11, 0.45, -0.02]
Lag having highest correlation:  4

-------Import every 50th row of BostonHousing dataset as a dataframe.
add='D:\\Downloads\\datasets\\BostonHousing.csv'
df=pd.read_csv(add,chunksize=50)
df2=pd.DataFrame()
for chunk in df:
    df2=df2.append(chunk.iloc[0,:])
print(df2.head())

output:
      age       b  chas     crim     dis  indus  lstat  medv    nox  ptratio  \
0    65.2  396.90   0.0  0.00632  4.0900   2.31   4.98  24.0  0.538     15.3   
50   45.7  395.56   0.0  0.08873  6.8147   5.64  13.45  19.7  0.439     16.8   
100  79.9  394.76   0.0  0.14866  2.7778   8.56   9.42  27.5  0.520     20.9   
150  97.3  372.80   0.0  1.65660  1.6180  19.58  14.10  21.5  0.871     14.7   
200  13.9  384.30   0.0  0.01778  7.6534   1.47   4.45  32.9  0.403     17.0   

     rad     rm    tax    zn  
0    1.0  6.575  296.0  18.0  
50   4.0  5.963  243.0  21.0  
100  5.0  6.727  384.0   0.0  
150  5.0  6.122  403.0   0.0  
200  3.0  7.135  402.0  95.0  

-------while importing change the 'medv' (median house value) column so that values < 25 
#becomes ‘Low’ and > 25 becomes ‘High’.
add='D:\\Downloads\\datasets\\BostonHousing.csv'
df=pd.read_csv(add,converters={'medv':lambda x:'High' if float(x)
                              >25 else 'Low'})
print(df.head())

output:
      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \
0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   
1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   
2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   
3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   
4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   

        b  lstat  medv  
0  396.90   4.98   Low  
1  396.90   9.14   Low  
2  392.83   4.03  High  
3  394.63   2.94  High  
4  396.90   5.33  High  

-------import only crim and medv from csv
add='D:\\Downloads\\datasets\\BostonHousing.csv'
df=pd.read_csv(add,usecols=['crim','medv'])
print(df.head())

output:
      crim  medv
0  0.00632  24.0
1  0.02731  21.6
2  0.02729  34.7
3  0.03237  33.4
4  0.06905  36.2

-------create a dataframe with rows as strides from a given series
L = pd.Series(range(15))
def gen_strides(a, stride_len=5, window_len=5):
    n_strides = ((a.size-window_len)//stride_len) + 1
    return np.array([a[s:(s+window_len)] for s in np.arange(0, a.size, stride_len)[:n_strides]])
gen_strides(L, stride_len=2, window_len=4)

output:
array([[ 0,  1,  2,  3],
       [ 2,  3,  4,  5],
       [ 4,  5,  6,  7],
       [ 6,  7,  8,  9],
       [ 8,  9, 10, 11],
       [10, 11, 12, 13]], dtype=int64)

-------Which manufacturer, model and type has the highest Price? What is the row and column number
add='D:\\Downloads\\datasets\\Cars93.csv'
df=pd.read_csv(add)
#print(df.loc[df.Price==np.max(df.Price),['Manufacturer','Model','Type']])
row,col=np.where(df.values==np.max(df.Price))
print(row,col)

output:
[58] [4]

-------Rename the column Type as CarType in df and replace the ‘.’ in column names with ‘_’
add='D:\\Downloads\\datasets\\Cars93.csv'
df=pd.read_csv(add)
#print(df.rename(columns={'Type':'Cartype'}))
df.columns.values[2]='CarType'
df.columns=df.columns.map(lambda x: x.replace('.','_'))
df.head()

output:

  Manufacturer    Model  CarType  Min_Price  Price  Max_Price  MPG_city  \
0        Acura  Integra    Small       12.9   15.9       18.8      25.0   
1          NaN   Legend  Midsize       29.2   33.9       38.7      18.0   
2         Audi       90  Compact       25.9   29.1       32.3      20.0   
3         Audi      100  Midsize        NaN   37.7       44.6      19.0   
4          BMW     535i  Midsize        NaN   30.0        NaN      22.0 

-------Count the number of missing values in each column of df. Which column has the maximum number of missing values?
add='D:\\Downloads\\datasets\\Cars93.csv'
df=pd.read_csv(add)
print(df.isnull().values.any()) # check for null values
print(df.apply(lambda x:x.isnull().sum())) 
print(np.argmax(df.apply(lambda x:x.isnull().sum())))

output:
True
Manufacturer           4
Model                  1
Type                   3
Min.Price              7
Price                  2
Max.Price              5
MPG.city               9
MPG.highway            2
AirBags                6
..........

-------Replace missing values in Min.Price and Max.Price columns with their respective mean.
add='D:\\Downloads\\datasets\\Cars93.csv'
df=pd.read_csv(add)
df_out=df[['Min.Price','Max.Price']].apply(lambda x:x.fillna(x.mean()))
print(df_out)

output:
    Min.Price  Max.Price
0   12.900000  18.800000
1   29.200000  38.700000
...... all the following rows....

-------use apply method to replace the missing values in Min.Price with the column’s mean and those in Max.Price with the column’s median.
add='D:\\Downloads\\datasets\\Cars93.csv'
d={'Min.Price':np.nanmean,'Max.Price':np.nanmedian}
df[['Min.Price','Max.Price']]=df[['Min.Price','Max.Price']].apply(lambda x,d:x.fillna(d[x.name](x)),args=(d,))
print(df[['Min.Price','Max.Price']])

output:
    Min.Price  Max.Price
0   12.900000  18.800000
1   29.200000  38.700000
...... all the following rows....

-------select a specific column from a dataframe as a dataframe
df=pd.DataFrame(np.arange(20).reshape(-1,5),columns=list('abcde')) #reshape(-1,5) is equivalent to reshape(4,5)
print(df)
#print((df.a)), #print(df['a']), #print(df.loc[:,'a'])
#print((df.iloc[:,1]))

output:
    a   b   c   d   e
0   0   1   2   3   4
1   5   6   7   8   9
2  10  11  12  13  14
3  15  16  17  18  19
0     0
1     5
2    10
3    15
Name: a, dtype: int32

-------
df=pd.DataFrame(np.arange(20).reshape(-1,5),columns=list('abcde')) #reshape(-1,5) is equivalent to reshape(4,5)
print(df)
print(df[list('cbade')]) # replace column c and a
#print(df[sorted(df.columns,reverse=True)]) # reverse order
print(df.sort_index(axis=1,ascending=False)) #reverse order

output:
    c   b   a   d   e
0   2   1   0   3   4
1   7   6   5   8   9
2  12  11  10  13  14
3  17  16  15  18  19
    e   d   c   b   a
0   4   3   2   1   0
1   9   8   7   6   5
2  14  13  12  11  10
3  19  18  17  16  15

-------Change the pandas display settings on printing the dataframe df it shows a maximum of 10 rows and 10 columns.
pd.set_option('display.max_columns',10)
pd.set_option('display.max_rows',10)

output:


-------
np.random.seed(2)
df=pd.DataFrame(np.random.random(4)**10,columns=['random'])
print(df)
print(df.round(4))
#print(df.apply(lambda x:'%.4f'%x,axis=1))
#print(df.applymap(lambda x:'%.4f'%x))
#pd.options.display.float_format='{:.4f}'.format
#print(df)
#pd.options.display.float_format=None

output:
         random
0  2.482071e-04
1  1.372126e-16
2  2.517450e-03
3  2.444050e-04
   random
0  0.0002
1  0.0000
2  0.0025
3  0.0002


-------Format the values in column 'random' of df as percentages.
np.random.seed(2)
df=pd.DataFrame(np.random.random(4),columns=['random'])
print(df)
out=df.style.format({'random':'{0:.2%}'.format})
out

output:
     random
0  0.435995
1  0.025926
2  0.549662
3  0.435322
random
0	43.60%
1	2.59%
2	54.97%
3	43.53%


-------filter the 'Manufacturer', 'Model' and 'Type' for every 20th row starting from 1st (row 0).
add='D:\\Downloads\\datasets\\Cars93.csv'
df=pd.read_csv(add)
print(df.iloc[::20,:][['Manufacturer','Model','Type']])

output:
   Manufacturer    Model     Type
0         Acura  Integra    Small
20     Chrysler  LeBaron  Compact
40        Honda  Prelude   Sporty
60      Mercury   Cougar  Midsize
80       Subaru   Loyale    Small

-------Replace NaNs with ‘missing’ in columns 'Manufacturer', 'Model' and 'Type' and create a index as a combination of these three columns and check if the index is a primary key.
add='D:\\Downloads\\datasets\\Cars93.csv'
df=pd.read_csv(add,usecols=[0,1,2,3,5])
df[['Manufacturer','Model','Type']]=df[['Manufacturer','Model','Type']].fillna('missing')
df.index=df.Manufacturer+'_'+df.Model+'_'+df.Type
print(df.index.is_unique)
print(df)

output:
True
                                 Manufacturer          Model     Type  Min.Price  Max.Price  
Acura_Integra_Small                     Acura        Integra    Small   12.9       18.8 
missing_Legend_Midsize                missing         Legend  Midsize   29.2       38.7  
Audi_90_Compact                          Audi             90  Compact   25.9       32.3 
Audi_100_Midsize                         Audi            100  Midsize   NaN        44.6  

-------row position of the 5th largest value of column 'a' in df
df=pd.DataFrame(np.random.randint(1,30,30).reshape(10,-1),columns=list('abc'))
print(df)
n=5
print(df['a'].argsort()[::-1][n])

output:
    a   b   c
0   9  16  14
1   9  23  12
2  19  12   9
3   8   3  18
4  12  22  16
5  27  21  29
6  21   6   8
7   4   7   5
8  11  12  20
9   8   7  11
8

-------find the position of the 2nd largest value greater than the mean
ser=pd.Series(np.random.randint(1,100,15))
print(ser)
print(ser.mean())
print(np.argwhere(ser>ser.mean())[1])

output:
0     41
1     16
2     73
3     23
4     44
5     83
6     76
7      8
8     35
9     50
10    96
11    76
12    86
13    48
14    64
dtype: int32
54.6
[5]

-------
df=pd.DataFrame(np.random.randint(10,40,60).reshape(-1,4))
#print(df)
rowsums=df.apply(np.sum,axis=1)
print(df.iloc[np.where(rowsums>100)[0][-2:],:])

output:
     0   1   2   3
12  36  25  27  17
14  36  25  19  23

-------do 54


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:


-------


output:




